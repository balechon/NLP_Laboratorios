{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a4c23490c0cac1",
   "metadata": {},
   "source": [
    "# Lab - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44a1977c794f50",
   "metadata": {},
   "source": [
    "## Integrantes\n",
    "- Jose Asitimbay\n",
    "- Brayan Lechon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d4af500f0e1e6",
   "metadata": {},
   "source": [
    "# Instrucciones\n",
    "Read the paper: \n",
    "\n",
    "https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "\n",
    "Implement a English-Spanish translator using transformers, use this tutorial for help:\n",
    "\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
    "\n",
    "Use the attention model from class to compare a few translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.117516Z",
     "start_time": "2024-09-10T03:32:22.109840Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# Descargar el tokenizador de NLTK si no está ya descargado\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_base')\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cac920b387af5",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ad382d715535ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.221943Z",
     "start_time": "2024-09-10T03:32:22.216638Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc00f2a1e2aeea43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.250616Z",
     "start_time": "2024-09-10T03:32:22.245377Z"
    }
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d42e85e5111d03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.266718Z",
     "start_time": "2024-09-10T03:32:22.263164Z"
    }
   },
   "outputs": [],
   "source": [
    "file = 'data/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e666a7feeafebf5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.306126Z",
     "start_time": "2024-09-10T03:32:22.301161Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    try:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH #and \\\n",
    "#            p[0].startswith(eng_prefixes)\n",
    "    except:\n",
    "        print(p)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df3876f118024ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.347288Z",
     "start_time": "2024-09-10T03:32:22.341236Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, file):\n",
    "    text = open(file, encoding='utf-8').read().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')][:2] for l in text ]\n",
    "    pairs = [pair for pair in pairs if len(pair) == 2]\n",
    "\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    \n",
    "    pairs = filterPairs(pairs)\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18efecccb19faa9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:29.272328Z",
     "start_time": "2024-09-10T03:32:22.409975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 12105\n",
      "spa 23411\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'spa', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f183711d79c3d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.909053Z",
     "start_time": "2024-09-10T03:32:29.273752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 12105\n",
      "spa 23411\n"
     ]
    }
   ],
   "source": [
    "input_lang1, output_lang1, pairs1 = prepareData('eng', 'spa', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d6a15b7a3d5b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.928820Z",
     "start_time": "2024-09-10T03:32:39.912066Z"
    }
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'spa', file)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9caf1a6557564e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.937912Z",
     "start_time": "2024-09-10T03:32:39.933763Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6343b5f7493919",
   "metadata": {},
   "source": [
    "# Implementacion del Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3ef3aa223e0d3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.977022Z",
     "start_time": "2024-09-10T03:32:39.939925Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(src.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.fc(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "317790cba828d2ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.993188Z",
     "start_time": "2024-09-10T03:32:39.980038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Función de entrenamiento (similar a la existente, pero adaptada para el transformer)\n",
    "def train_epoch(dataloader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_tensor, target_tensor = batch\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(input_tensor, target_tensor[:, :-1])\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target = target_tensor[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Función de evaluación\n",
    "def evaluate(model, sentence, input_lang, output_lang):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence).unsqueeze(0)\n",
    "        output_tensor = torch.LongTensor([[SOS_token]]).to(device)\n",
    "        \n",
    "        for _ in range(MAX_LENGTH):\n",
    "            predictions = model(input_tensor, output_tensor)\n",
    "            _, predicted_id = predictions[:, -1:].max(2)\n",
    "            \n",
    "            if predicted_id.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "            output_tensor = torch.cat([output_tensor, predicted_id], dim=1)\n",
    "\n",
    "        decoded_words = [output_lang.index2word[token.item()] for token in output_tensor[0][1:]]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630d0162fb120596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:40.015712Z",
     "start_time": "2024-09-10T03:32:39.996632Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def train(dataloader, model, n_epochs, learning_rate, print_every=1, plot_every=1):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(dataloader, model, optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "768f491c66cd14f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:36:05.508133Z",
     "start_time": "2024-09-10T03:32:40.018725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 12105\n",
      "spa 23411\n",
      "4m 59s (- 0m 0s) (5 100%) 2.6586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Hiperparámetros\n",
    "src_vocab_size = input_lang.n_words\n",
    "tgt_vocab_size = output_lang.n_words\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "d_ff = 512\n",
    "max_seq_length = MAX_LENGTH\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Inicializar el modelo\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Mover el modelo al dispositivo\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# Entrenamiento\n",
    "n_epochs = 5\n",
    "print_every = 5\n",
    "plot_every = 5\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "train_dataloader = get_dataloader(32)[2]  # Asumiendo que esta función ya está definida\n",
    "\n",
    "train(train_dataloader, transformer, n_epochs, learning_rate, print_every, plot_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1668c6e78dabc65a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:36:05.511157Z",
     "start_time": "2024-09-10T03:36:05.510267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluación\n",
    "def evaluate(model, sentence, input_lang, output_lang):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence).unsqueeze(0)\n",
    "        output_tensor = torch.LongTensor([[SOS_token]]).to(device)\n",
    "        \n",
    "        for _ in range(MAX_LENGTH):\n",
    "            predictions = model(input_tensor, output_tensor)\n",
    "            _, predicted_id = predictions[:, -1:].max(2)\n",
    "            \n",
    "            if predicted_id.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "            output_tensor = torch.cat([output_tensor, predicted_id], dim=1)\n",
    "\n",
    "        decoded_words = [output_lang.index2word[token.item()] for token in output_tensor[0][1:]]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f30805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando ejemplos aleatorios:\n",
      "> the sun disappeared behind the clouds\n",
      "= el sol desaparecio detras de las nubes\n",
      "< SOS el sol detras del nubes\n",
      "\n",
      "> tom could never forget the terror of war\n",
      "= tom nunca pudo olvidar el horror de la guerra\n",
      "< SOS tom nunca podia contener la guerra de la guerra\n",
      "\n",
      "> everything tom does is controversial\n",
      "= todo lo que tom hace es controversial\n",
      "< SOS tom es controversial\n",
      "\n",
      "> please let me sleep\n",
      "= dejame dormir por favor\n",
      "< SOS dormir por favor\n",
      "\n",
      "> they will come\n",
      "= vienen\n",
      "< SOS\n",
      "\n",
      "> my daughter has braces\n",
      "= mi hija tiene frenillos\n",
      "< SOS mi hija tiene muchos libros\n",
      "\n",
      "> we re still on our honeymoon\n",
      "= todavia estamos en nuestra luna de miel\n",
      "< SOS nuestros gastos\n",
      "\n",
      "> she wrote in ink\n",
      "= ella escribio con tinta\n",
      "< SOS en tinta\n",
      "\n",
      "> tom was shot by a firing squad\n",
      "= tom fue fusilado por un peloton de fusilamiento\n",
      "< SOS tom fue atropellado por un grave\n",
      "\n",
      "> the dog was in the box under the table\n",
      "= el perro estaba en la caja bajo la mesa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< SOS la caja estaba en la caja bajo la mesa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def evaluate(model, sentence, input_lang, output_lang, max_length=MAX_LENGTH):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence).to(device)\n",
    "        target_tensor = torch.LongTensor([[SOS_token]]).to(device)\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output = model(input_tensor, target_tensor)\n",
    "            topv, topi = output[:, -1].topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "            target_tensor = torch.cat([target_tensor, topi.detach()], dim=1)\n",
    "\n",
    "        decoded_words = []\n",
    "        for iti in range(target_tensor.size(1)):\n",
    "            if target_tensor[0][iti].item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[target_tensor[0][iti].item()])\n",
    "    \n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, pairs, input_lang, output_lang, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(model, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "def evaluateAndShowAttention(model, sentence, input_lang, output_lang):\n",
    "    output_words = evaluate(model, sentence, input_lang, output_lang)\n",
    "    print('input =', sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "\n",
    "# Uso de las funciones de evaluación\n",
    "print(\"Evaluando ejemplos aleatorios:\")\n",
    "evaluateRandomly(transformer, pairs, input_lang, output_lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f114f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(transformer.state_dict(), 'transformer_test_5_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8f074ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = where is the library ?\n",
      "output = SOS es la biblioteca ?\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(transformer, \"where is the library ?\", input_lang, output_lang)\n",
    "# tengo un error no envez de la primera palabra me da SOS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
