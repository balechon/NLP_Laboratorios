{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a4c23490c0cac1",
   "metadata": {},
   "source": [
    "# Lab - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44a1977c794f50",
   "metadata": {},
   "source": [
    "## Integrantes\n",
    "- Jose Asitimbay\n",
    "- Brayan Lechon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d4af500f0e1e6",
   "metadata": {},
   "source": [
    "# Instrucciones\n",
    "Read the paper: \n",
    "\n",
    "https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "\n",
    "Implement a English-Spanish translator using transformers, use this tutorial for help:\n",
    "\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
    "\n",
    "Use the attention model from class to compare a few translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.117516Z",
     "start_time": "2024-09-10T03:32:22.109840Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# Descargar el tokenizador de NLTK si no est√° ya descargado\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_base')\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cac920b387af5",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ad382d715535ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.221943Z",
     "start_time": "2024-09-10T03:32:22.216638Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc00f2a1e2aeea43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.250616Z",
     "start_time": "2024-09-10T03:32:22.245377Z"
    }
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d42e85e5111d03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.266718Z",
     "start_time": "2024-09-10T03:32:22.263164Z"
    }
   },
   "outputs": [],
   "source": [
    "file = 'data/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e666a7feeafebf5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.306126Z",
     "start_time": "2024-09-10T03:32:22.301161Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    try:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH #and \\\n",
    "#            p[0].startswith(eng_prefixes)\n",
    "    except:\n",
    "        print(p)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df3876f118024ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:22.347288Z",
     "start_time": "2024-09-10T03:32:22.341236Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, file):\n",
    "    text = open(file, encoding='utf-8').read().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')][:2] for l in text ]\n",
    "    pairs = [pair for pair in pairs if len(pair) == 2]\n",
    "\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    \n",
    "    pairs = filterPairs(pairs)\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18efecccb19faa9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:29.272328Z",
     "start_time": "2024-09-10T03:32:22.409975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 12105\n",
      "spa 23411\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'spa', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f183711d79c3d18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.909053Z",
     "start_time": "2024-09-10T03:32:29.273752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 12105\n",
      "spa 23411\n"
     ]
    }
   ],
   "source": [
    "input_lang1, output_lang1, pairs1 = prepareData('eng', 'spa', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d6a15b7a3d5b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.928820Z",
     "start_time": "2024-09-10T03:32:39.912066Z"
    }
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'spa', file)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9caf1a6557564e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.937912Z",
     "start_time": "2024-09-10T03:32:39.933763Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6343b5f7493919",
   "metadata": {},
   "source": [
    "# Implementacion del Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3ef3aa223e0d3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.977022Z",
     "start_time": "2024-09-10T03:32:39.939925Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(src.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.fc(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "317790cba828d2ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:39.993188Z",
     "start_time": "2024-09-10T03:32:39.980038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Funci√≥n de entrenamiento (similar a la existente, pero adaptada para el transformer)\n",
    "def train_epoch(dataloader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_tensor, target_tensor = batch\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(input_tensor, target_tensor[:, :-1])\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        target = target_tensor[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Funci√≥n de evaluaci√≥n\n",
    "def evaluate(model, sentence, input_lang, output_lang):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence).unsqueeze(0)\n",
    "        output_tensor = torch.LongTensor([[SOS_token]]).to(device)\n",
    "        \n",
    "        for _ in range(MAX_LENGTH):\n",
    "            predictions = model(input_tensor, output_tensor)\n",
    "            _, predicted_id = predictions[:, -1:].max(2)\n",
    "            \n",
    "            if predicted_id.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "            output_tensor = torch.cat([output_tensor, predicted_id], dim=1)\n",
    "\n",
    "        decoded_words = [output_lang.index2word[token.item()] for token in output_tensor[0][1:]]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630d0162fb120596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:32:40.015712Z",
     "start_time": "2024-09-10T03:32:39.996632Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def train(dataloader, model, n_epochs, learning_rate, print_every=1, plot_every=1):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(dataloader, model, optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                         epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "768f491c66cd14f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:36:05.508133Z",
     "start_time": "2024-09-10T03:32:40.018725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 12105\n",
      "spa 23411\n",
      "4m 59s (- 0m 0s) (5 100%) 2.6586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Hiperpar√°metros\n",
    "src_vocab_size = input_lang.n_words\n",
    "tgt_vocab_size = output_lang.n_words\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "d_ff = 512\n",
    "max_seq_length = MAX_LENGTH\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Inicializar el modelo\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Mover el modelo al dispositivo\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "# Entrenamiento\n",
    "n_epochs = 5\n",
    "print_every = 5\n",
    "plot_every = 5\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "train_dataloader = get_dataloader(32)[2]  # Asumiendo que esta funci√≥n ya est√° definida\n",
    "\n",
    "train(train_dataloader, transformer, n_epochs, learning_rate, print_every, plot_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1668c6e78dabc65a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T03:36:05.511157Z",
     "start_time": "2024-09-10T03:36:05.510267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluaci√≥n\n",
    "def evaluate(model, sentence, input_lang, output_lang):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence).unsqueeze(0)\n",
    "        output_tensor = torch.LongTensor([[SOS_token]]).to(device)\n",
    "        \n",
    "        for _ in range(MAX_LENGTH):\n",
    "            predictions = model(input_tensor, output_tensor)\n",
    "            _, predicted_id = predictions[:, -1:].max(2)\n",
    "            \n",
    "            if predicted_id.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "            output_tensor = torch.cat([output_tensor, predicted_id], dim=1)\n",
    "\n",
    "        decoded_words = [output_lang.index2word[token.item()] for token in output_tensor[0][1:]]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f30805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando ejemplos aleatorios:\n",
      "> the sun disappeared behind the clouds\n",
      "= el sol desaparecio detras de las nubes\n",
      "< SOS el sol detras del nubes\n",
      "\n",
      "> tom could never forget the terror of war\n",
      "= tom nunca pudo olvidar el horror de la guerra\n",
      "< SOS tom nunca podia contener la guerra de la guerra\n",
      "\n",
      "> everything tom does is controversial\n",
      "= todo lo que tom hace es controversial\n",
      "< SOS tom es controversial\n",
      "\n",
      "> please let me sleep\n",
      "= dejame dormir por favor\n",
      "< SOS dormir por favor\n",
      "\n",
      "> they will come\n",
      "= vienen\n",
      "< SOS\n",
      "\n",
      "> my daughter has braces\n",
      "= mi hija tiene frenillos\n",
      "< SOS mi hija tiene muchos libros\n",
      "\n",
      "> we re still on our honeymoon\n",
      "= todavia estamos en nuestra luna de miel\n",
      "< SOS nuestros gastos\n",
      "\n",
      "> she wrote in ink\n",
      "= ella escribio con tinta\n",
      "< SOS en tinta\n",
      "\n",
      "> tom was shot by a firing squad\n",
      "= tom fue fusilado por un peloton de fusilamiento\n",
      "< SOS tom fue atropellado por un grave\n",
      "\n",
      "> the dog was in the box under the table\n",
      "= el perro estaba en la caja bajo la mesa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< SOS la caja estaba en la caja bajo la mesa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def evaluate(model, sentence, input_lang, output_lang, max_length=MAX_LENGTH):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence).to(device)\n",
    "        target_tensor = torch.LongTensor([[SOS_token]]).to(device)\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output = model(input_tensor, target_tensor)\n",
    "            topv, topi = output[:, -1].topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "            target_tensor = torch.cat([target_tensor, topi.detach()], dim=1)\n",
    "\n",
    "        decoded_words = []\n",
    "        for iti in range(target_tensor.size(1)):\n",
    "            if target_tensor[0][iti].item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[target_tensor[0][iti].item()])\n",
    "    \n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, pairs, input_lang, output_lang, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(model, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "def evaluateAndShowAttention(model, sentence, input_lang, output_lang):\n",
    "    output_words = evaluate(model, sentence, input_lang, output_lang)\n",
    "    print('input =', sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "\n",
    "# Uso de las funciones de evaluaci√≥n\n",
    "print(\"Evaluando ejemplos aleatorios:\")\n",
    "evaluateRandomly(transformer, pairs, input_lang, output_lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f114f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(transformer.state_dict(), 'transformer_test_5_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8f074ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = where is the library ?\n",
      "output = SOS es la biblioteca ?\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(transformer, \"where is the library ?\", input_lang, output_lang)\n",
    "# tengo un error no envez de la primera palabra me da SOS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
